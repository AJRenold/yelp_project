{
 "metadata": {
  "name": "Ensemble_test"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import json\n",
      "from itertools import islice\n",
      "from collections import Counter, defaultdict\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "import re\n",
      "import nltk\n",
      "\n",
      "# sklean\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.linear_model import LinearRegression\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "# our bayes\n",
      "from bayes import NaiveBayes\n",
      "\n",
      "# news group data\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "\n",
      "print os.getcwd()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Users/ajrenold/Dropbox/iSchool/2013Spring/DataMining/yelp_project\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file_dict = {}\n",
      "file_dict['reviews'] = 'yelp_academic_dataset_review.json'\n",
      "file_dict['stopwords'] = 'stop-words-english3-google.txt'\n",
      "\n",
      "review_file = open(file_dict['reviews'])\n",
      "review_file_s = islice(review_file,None)\n",
      "print review_file_s\n",
      "print file_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<itertools.islice object at 0x104b9dc00>\n",
        "{'reviews': 'yelp_academic_dataset_review.json', 'stopwords': 'stop-words-english3-google.txt'}\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(file_dict['stopwords'])\n",
      "print f\n",
      "\n",
      "stops = {}\n",
      "\n",
      "for line in islice(f,None):\n",
      "    word = line.lower().strip()\n",
      "    if word not in stops:\n",
      "        stops[word] = True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<open file 'stop-words-english3-google.txt', mode 'r' at 0x104b9f810>\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "review_json = open(file_dict['reviews'])\n",
      "reviews_for_df = [ json.loads(line) for line in review_file_s ]\n",
      "review_df = pd.DataFrame(reviews_for_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "review_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Int64Index: 229907 entries, 0 to 229906\n",
        "Data columns:\n",
        "business_id    229907  non-null values\n",
        "date           229907  non-null values\n",
        "review_id      229907  non-null values\n",
        "stars          229907  non-null values\n",
        "text           229907  non-null values\n",
        "type           229907  non-null values\n",
        "user_id        229907  non-null values\n",
        "votes          229907  non-null values\n",
        "dtypes: int64(1), object(7)"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# extract useful_votes from votes column\n",
      "\n",
      "#review_df['tokens'] = review_df['text'].apply(nltk.word_tokenize) # tokenize using nltk\n",
      "review_df['useful_votes'] = review_df['votes'].apply(lambda x:x['useful']) # extract useful votes\n",
      "review_df = review_df.set_index(['review_id']) # index on review_id\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# partition data to train and test\n",
      "train = []\n",
      "test = []\n",
      "for row in islice(review_df[['text','useful_votes']].iterrows(),None):\n",
      "    train_or_test = np.random.uniform()\n",
      "    # partition training data\n",
      "    if train_or_test < 0.8:\n",
      "        train.append(row[1])\n",
      "    # partition testing data\n",
      "    else:\n",
      "        test.append(row[1])\n",
      "        \n",
      "train_df = pd.DataFrame(train)\n",
      "test_df = pd.DataFrame(test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Index: 183795 entries, IjZ33sJrzXqU-0X6U8NwyA to QM1rFJsW-ZJoCHbgsysKaw\n",
        "Data columns:\n",
        "text            183795  non-null values\n",
        "useful_votes    183795  non-null values\n",
        "dtypes: int64(1), object(1)"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Index: 46112 entries, fWKvX83p0-ka4JS3dc6E5A to f9JaiNg_FMoPNWxt7MlbZQ\n",
        "Data columns:\n",
        "text            46112  non-null values\n",
        "useful_votes    46112  non-null values\n",
        "dtypes: int64(1), object(1)"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get reviews with useful votes from training data\n",
      "useful_review_df = train_df[train_df['useful_votes']>1] # 40 percent votes are 0 , 28 % are 1 , discarding those\n",
      "useful_review_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Index: 55374 entries, G-WvGaISbqqaMHlNnByodA to z5b2p5TbCg0uaIiIe8n62w\n",
        "Data columns:\n",
        "text            55374  non-null values\n",
        "useful_votes    55374  non-null values\n",
        "dtypes: int64(1), object(1)"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tokenize(text):\n",
      "    text = re.sub(r\"[\\n\\.,;\\!\\?\\(\\)\\[\\]\\*/:+\\-\\~]\",\" \",text.lower())\n",
      "    text = re.sub(r\"(\\b[\\d]+\\b|\\b[\\d]+[a-z]+\\b)\",\" \",text)\n",
      "    #inplist = nltk.word_tokenize(text)    \n",
      "    inplist = text.split(' ')\n",
      "    finallist = list()\n",
      "    result = list()\n",
      "    # wordcorrect for tokens\n",
      "    #finallist = wordcorrect(inplist)\n",
      "    finallist = inplist\n",
      "    \n",
      "    for i in range(len(finallist)):        \n",
      "        if stops.has_key(inplist[i]): # remove stop words\n",
      "            continue\n",
      "        elif '$' in inplist[i]:\n",
      "            result.append('priceMention')\n",
      "        else:\n",
      "            result.append(inplist[i])\n",
      "    \n",
      "    return result\n",
      "\n",
      "extr_feat = list() # global extracted features list\n",
      "\n",
      "def tfidf_vectorize():\n",
      "    vectorizer1 = TfidfVectorizer(tokenizer = tokenize)\n",
      "    X1 = vectorizer1.fit_transform(useful_review_df['text'][:10000]) # sparse matrix with tfidf weights\n",
      "    features = vectorizer1.get_feature_names()\n",
      "    #print features\n",
      "    #print vectorizer1.get_feature_names()\n",
      "    X1 = X1.toarray() # convert into 2d array \n",
      "    \n",
      "    for i in range(len(X1)):\n",
      "        \n",
      "        median = numpy.median(X1[i][np.nonzero(X1[i])]) # extract features with top 50 % tfidf weights\n",
      "        std = numpy.std(X1[i][np.nonzero(X1[i])])\n",
      "        for x in range(len(X1[i])):\n",
      "            if X1[i][x] >= median + (2*std):\n",
      "                #print features[x]\n",
      "                global extr_feat\n",
      "                try:\n",
      "                    if extr_feat.index(features[x]):\n",
      "                        continue\n",
      "                except ValueError:\n",
      "                    extr_feat.append(features[x])\n",
      "               \n",
      "         \n",
      "    #vectorizer = CountVectorizer(vocabulary = extr_feat)\n",
      "    #trnginp = vectorizer.fit_transform(corpus)\n",
      "    #print vectorizer.get_feature_names()\n",
      "    #print trnginp.toarray()\n",
      "    \n",
      "\n",
      "\n",
      "count_vect = None\n",
      "trnginp = None\n",
      "\n",
      "def count_vectorize():\n",
      "    global count_vect\n",
      "    global trnginp\n",
      "    global extr_feat\n",
      "    count_vect = CountVectorizer(tokenizer = tokenize,vocabulary = extr_feat, min_df=30)\n",
      "    trnginp = count_vect.fit_transform(useful_review_df['text'][:10000])\n",
      "\n",
      "\n",
      "%time tfidf_vectorize() # 482 s \n",
      "%time count_vectorize() # 3.97 s\n",
      "\n",
      "#print result.ndim\n",
      "#print count_vect.vocabulary_\n",
      "#dir(trnginp)\n",
      "#print trnginp.shape\n",
      "#.get_feature_names()\n",
      "\n",
      "#count_vect = CountVectorizer(input = inpcorpus, tokenizer = tokenize)\n",
      "\n",
      "\n",
      "#vectop = count_vect.fit_transform(review_df['text'][:3])\n",
      "\n",
      "#cnt = 0\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 482.62 s, sys: 2.11 s, total: 484.73 s\n",
        "Wall time: 484.31 s\n",
        "CPU times: user 3.93 s, sys: 0.04 s, total: 3.98 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 3.97 s\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#### Re-write to optimize time\n",
      "#### NOT YET FINISHED\n",
      "\n",
      "def tokenize(text):\n",
      "    text = re.sub(r\"[\\n\\.,;\\!\\?\\(\\)\\[\\]\\*/:+\\-\\~]\",\" \",text.lower())\n",
      "    text = re.sub(r\"(\\b[\\d]+\\b|\\b[\\d]+[a-z]+\\b)\",\" \",text)\n",
      "    #inplist = nltk.word_tokenize(text)    \n",
      "    inplist = text.split(' ')\n",
      "    finallist = list()\n",
      "    result = list()\n",
      "    # wordcorrect for tokens\n",
      "    #finallist = wordcorrect(inplist)\n",
      "    finallist = inplist\n",
      "    \n",
      "    for i in range(len(finallist)):        \n",
      "        if stops.has_key(inplist[i]): # remove stop words\n",
      "            continue\n",
      "        elif '$' in inplist[i]:\n",
      "            result.append('priceMention')\n",
      "        else:\n",
      "            result.append(inplist[i])\n",
      "    \n",
      "    return result\n",
      "\n",
      "def tfidf_vectorize(text):\n",
      "    extr_features = list()\n",
      "    vectorizer1 = TfidfVectorizer(tokenizer = tokenize)\n",
      "    X1 = vectorizer1.fit_transform(text) # sparse matrix with tfidf weights\n",
      "    features = vectorizer1.get_feature_names()\n",
      "    #print features\n",
      "    #print vectorizer1.get_feature_names()\n",
      "    X1 = X1.toarray() # convert into 2d array \n",
      "    for i in range(len(X1)):\n",
      "        \n",
      "        median = numpy.median(X1[i][np.nonzero(X1[i])]) # extract features with top 50 % tfidf weights\n",
      "        std = numpy.std(X1[i][np.nonzero(X1[i])])\n",
      "        for x in range(len(X1[i])):\n",
      "            if X1[i][x] >= median + (2*std):\n",
      "                #print features[x]\n",
      "                try:\n",
      "                    if extr_features.index(features[x]):\n",
      "                        continue\n",
      "                except ValueError:\n",
      "                    extr_features.append(features[x])\n",
      "                    \n",
      "    return extr_features\n",
      "               \n",
      "def create_training_input(text):\n",
      "    extracted_features = tfidf_vectorize(text)\n",
      "    count_vect = CountVectorizer(tokenizer = tokenize,vocabulary = extracted_features, min_df=30)\n",
      "    return count_vect, count_vect.fit_transform(text)\n",
      "\n",
      "%time count_vect, training_input = create_training_input(useful_review_df['text'][:10000])\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 506.25 s, sys: 2.52 s, total: 508.77 s\n",
        "Wall time: 510.36 s\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train LRModel\n",
      "trnglabels = useful_review_df['useful_votes'][:10000]\n",
      "LRModel = LinearRegression()\n",
      "%time LRModel.fit(trnginp,trnglabels)\n",
      "modelpar = LRModel.coef_\n",
      "print min(modelpar),max(modelpar), modelpar"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 21.47 s, sys: 0.07 s, total: 21.53 s\n",
        "Wall time: 21.83 s\n",
        "-31.3887246532 25.8937891885 [ 0.0154817   0.01803503  0.10620435 ..., -0.29255836 -0.4994099   1.7568387 ]\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test LRModel on its own\n",
      "testrange = count_vect.fit_transform(test_df['text'][:])\n",
      "pred_results = LRModel.predict(testrange.toarray())\n",
      "res = zip(test_df['useful_votes'][:].values,pred_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# LRModel RMSE\n",
      "errors = []\n",
      "for r in res:\n",
      "    err = abs(r[0]-r[1])\n",
      "    errors.append(err)\n",
      "    \n",
      "print np.sqrt(np.mean([ e**2 for e in errors ]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4.96354971244\n"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create labels for NaiveBayes\n",
      "train_labels = []\n",
      "for useful_votes in train_df['useful_votes'][:]:\n",
      "    if useful_votes == 0:\n",
      "        train_labels.append('0')\n",
      "    elif useful_votes < 3:\n",
      "        train_labels.append('1')\n",
      "    elif useful_votes >= 3 and useful_votes < 6:\n",
      "        train_labels.append('2')\n",
      "    else:\n",
      "        train_labels.append('3')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train NaiveBayes\n",
      "% time clfr = NaiveBayes(train_df['text'], train_labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'1': 0.41984275959628936, '0': 0.4150874615740363, '3': 0.04312413286542071, '2': 0.12194564596425365}\n",
        "141625"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "CPU times: user 64.17 s, sys: 1.73 s, total: 65.90 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 65.60 s\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "matches = defaultdict(Counter)\n",
      "results = []\n",
      "\n",
      "for i,item in enumerate(test_df['text']):\n",
      "    label = clfr.label_new(item)\n",
      "    if label[0][1] == '1':\n",
      "        results.append((1.5,test_df['useful_votes'][i]))\n",
      "    elif label[0][1] == '2':\n",
      "        results.append((4.5,test_df['useful_votes'][i]))\n",
      "    elif label[0][1] == '3':\n",
      "        results.append((7.5,test_df['useful_votes'][i]))\n",
      "        #testrange = count_vect.fit_transform([item])\n",
      "        #votes = LRModel.predict(testrange.toarray())\n",
      "        #results.append((votes[0],test_df['useful_votes'][i]))\n",
      "    else:\n",
      "        results.append((0,test_df['useful_votes'][i]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 68,
       "text": [
        "[(1.5, 5),\n",
        " (4.5, 1),\n",
        " (0, 1),\n",
        " (0, 0),\n",
        " (4.5, 3),\n",
        " (0, 0),\n",
        " (0, 1),\n",
        " (4.5, 2),\n",
        " (4.5, 2),\n",
        " (4.5, 3)]"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Labeling RMSE\n",
      "errors = []\n",
      "for r in results:\n",
      "    err = abs(r[0]-r[1])\n",
      "    errors.append(err)\n",
      "\n",
      "print np.sqrt(np.mean([ e**2 for e in errors ]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3.37609056583\n"
       ]
      }
     ],
     "prompt_number": 69
    }
   ],
   "metadata": {}
  }
 ]
}